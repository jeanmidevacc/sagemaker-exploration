{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import ast\n",
    "import itertools\n",
    "from collections import Counter \n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mth\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_process = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "dfp_ml = pd.read_csv(f'{os.environ[\"AWS_SAGEMAKER_S3_LOCATION\"]}/data/dataset_ml.csv')\n",
    "\n",
    "# wrokin on sample for the scoring set\n",
    "dfp_toscore = pd.read_csv(f'{os.environ[\"AWS_SAGEMAKER_S3_LOCATION\"]}/data/dataset_toscore.csv').sample(frac=0.1).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_ml['deck'] = dfp_ml['cards'].apply(lambda cards:ast.literal_eval(cards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training set and the testing set\n",
    "dfp_train, dfp_test = train_test_split(dfp_ml, test_size=0.20, random_state=0)\n",
    "dfp_train.reset_index(drop=True,inplace=True)\n",
    "dfp_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the popularcard for a specific archetype\n",
    "def compute_cards_count(decks):\n",
    "    cards_count = {}\n",
    "    for deck in decks:\n",
    "        cards = ast.literal_eval(deck)\n",
    "        for card in cards:\n",
    "            if card in cards_count:\n",
    "                cards_count[card] += 1\n",
    "            else:\n",
    "                cards_count[card] = 1    \n",
    "    return cards_count\n",
    "\n",
    "def compute_cards_count_v2(decks):\n",
    "    return dict(Counter(list(itertools.chain.from_iterable(decks))))\n",
    "\n",
    "def compute_ranking_cards(cards_count,k=5):\n",
    "    dfp_ranking = pd.DataFrame.from_dict(cards_count, orient='index',columns=['count']).sort_values('count', ascending=False)\n",
    "    return dfp_ranking['count'].tolist()[:k]\n",
    "\n",
    "dfp_train_agg = dfp_train.groupby(['archetype'])['deck'].apply(list).to_frame()\n",
    "dfp_train_agg['cards_count'] = dfp_train_agg['deck'].apply(lambda deck: compute_cards_count_v2(deck))\n",
    "dfp_train_agg['cards_selection'] = dfp_train_agg['cards_count'].apply(lambda cards_count: compute_ranking_cards(cards_count))\n",
    "\n",
    "cards_selection = list(dict.fromkeys(list(itertools.chain.from_iterable(dfp_train_agg['cards_selection'].tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the deck\n",
    "def encode_deck(deck, cards_selection):\n",
    "    encoded_deck = [0] * len(cards_selection)\n",
    "    \n",
    "    for card in deck:\n",
    "        if card in cards_selection:\n",
    "            idx = cards_selection.index(card)\n",
    "            encoded_deck[idx] += 1\n",
    "    return encoded_deck\n",
    "\n",
    "dfp_train['encoded_deck'] = dfp_train['deck'].apply(lambda deck: encode_deck(deck, cards_selection))\n",
    "dfp_test['encoded_deck'] = dfp_test['deck'].apply(lambda deck: encode_deck(deck, cards_selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an indexer to encode the hero\n",
    "encoder_hero = preprocessing.LabelEncoder()\n",
    "\n",
    "heroes = list(dfp_train['hero'].unique())\n",
    "encoder_hero.fit(heroes)\n",
    "\n",
    "dfp_train['encoded_hero'] = encoder_hero.transform(dfp_train['hero'].to_list())\n",
    "dfp_test['encoded_hero'] = encoder_hero.transform(dfp_test['hero'].to_list())\n",
    "\n",
    "# build the feature vector\n",
    "dfp_train['features'] = dfp_train.apply(lambda row: [row['encoded_hero']] + row['encoded_deck'], axis=1)\n",
    "dfp_test['features'] = dfp_test.apply(lambda row: [row['encoded_hero']] + row['encoded_deck'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build an indexer for the output to predict\n",
    "encoder_archetype = preprocessing.LabelEncoder()\n",
    "\n",
    "archetypes = list(dfp_train['archetype'].unique())\n",
    "encoder_archetype.fit(archetypes)\n",
    "\n",
    "dfp_train['label'] = encoder_archetype.transform(dfp_train['archetype'].to_list())\n",
    "dfp_test['label'] = encoder_archetype.transform(dfp_test['archetype'].to_list())\n",
    "\n",
    "dfp_train_rtu = dfp_train[['label', 'features']]\n",
    "dfp_test_rtu = dfp_test[['label', 'features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset to score\n",
    "dfp_toscore['deck'] = dfp_toscore['cards'].apply(lambda cards:ast.literal_eval(cards))\n",
    "dfp_toscore['encoded_deck'] = dfp_toscore['deck'].apply(lambda deck: encode_deck(deck, cards_selection))\n",
    "dfp_toscore['encoded_hero'] = encoder_hero.transform(dfp_toscore['hero'].to_list())\n",
    "dfp_toscore['features'] = dfp_toscore.apply(lambda row: [row['encoded_hero']] + row['encoded_deck'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_process = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth=2, random_state=0).fit(dfp_train_rtu['features'].tolist(), dfp_train_rtu['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_train = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_evaluate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_at_k(recommendations, item, k=5):\n",
    "    if item in recommendations[:k]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def get_ndcg_at_k(recommendations, item, k=5):\n",
    "    for idx, elt in enumerate(recommendations[:k]):\n",
    "        if item == elt:\n",
    "            return mth.log(2) / mth.log(idx+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_test_rtu['prediction'] = dfp_test_rtu['features'].apply(lambda features: model.predict_proba([features])[0])\n",
    "dfp_test_rtu['prediction'] = dfp_test_rtu['prediction'].apply(lambda prediction: np.argsort(prediction)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for k in [1,3,5]:\n",
    "    dfp_test_rtu[f'precision_at_{k}'] = dfp_test_rtu.apply(lambda row: get_precision_at_k(row['prediction'], row['label'], k), axis=1)\n",
    "    metrics.append(f'precision_at_{k}')\n",
    "    if k > 1:\n",
    "        dfp_test_rtu[f'ndcg_at_{k}'] = dfp_test_rtu.apply(lambda row: get_ndcg_at_k(row['prediction'], row['label'], k), axis=1)\n",
    "        metrics.append(f'ndcg_at_{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision_at_1    0.134409\n",
       "precision_at_3    0.267022\n",
       "ndcg_at_3         0.211953\n",
       "precision_at_5    0.360951\n",
       "ndcg_at_5         0.249772\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp_test_rtu[metrics].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_evaluate = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_score = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the unknow \n",
    "dfp_toscore['prediction_prob'] = dfp_toscore['features'].apply(lambda features: model.predict_proba([features])[0])\n",
    "dfp_toscore['predictions'] = dfp_toscore['prediction_prob'].apply(lambda prediction: np.argsort(prediction)[::-1])\n",
    "\n",
    "labels = list(model.classes_)\n",
    "dfp_toscore['predictions'] = dfp_toscore['predictions'].apply(lambda predictions: [labels[idx] for idx in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_score = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_date = datetime.utcnow()\n",
    "dict_run = {\n",
    "    'execution_date': execution_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'time_process' : toc_process - tic_process,\n",
    "    'time_train' : toc_train - tic_train,\n",
    "    'time_evaluate' : toc_evaluate - tic_evaluate,\n",
    "    'time_score' : toc_score - tic_score\n",
    "}\n",
    "\n",
    "filename = execution_date.strftime('%Y%m%d_%H-%M-%S')\n",
    "with open(f'data/{filename}.json', 'w') as fp:\n",
    "    json.dump(dict_run, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script mostly inspired by this tutorials https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/scikit_learn_data_processing_and_model_evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jobs/0_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jobs/0_process.py\n",
    "\n",
    "\"\"\"\n",
    "Script to process the raw data\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "from collections import Counter \n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def compute_cards_count(decks):\n",
    "    cards_count = {}\n",
    "    for deck in decks:\n",
    "        cards = ast.literal_eval(deck)\n",
    "        for card in cards:\n",
    "            if card in cards_count:\n",
    "                cards_count[card] += 1\n",
    "            else:\n",
    "                cards_count[card] = 1    \n",
    "    return cards_count\n",
    "\n",
    "def compute_cards_count_v2(decks):\n",
    "    return dict(Counter(list(itertools.chain.from_iterable(decks))))\n",
    "\n",
    "def compute_ranking_cards(cards_count,k=5):\n",
    "    dfp_ranking = pd.DataFrame.from_dict(cards_count, orient='index',columns=['count']).sort_values('count', ascending=False)\n",
    "    return dfp_ranking['count'].tolist()[:k]\n",
    "\n",
    "def encode_deck(deck, cards_selection):\n",
    "    encoded_deck = [0] * len(cards_selection)\n",
    "    \n",
    "    for card in deck:\n",
    "        if card in cards_selection:\n",
    "            idx = cards_selection.index(card)\n",
    "            encoded_deck[idx] += 1\n",
    "    return encoded_deck\n",
    "\n",
    "def prepare_dataset_to_train(dfp, cards_selection, encoder_hero):\n",
    "    dfp['encoded_deck'] = dfp['deck'].apply(lambda deck: encode_deck(deck, cards_selection))\n",
    "    dfp['encoded_hero'] = encoder_hero.transform(dfp['hero'].to_list())\n",
    "    dfp['features'] = dfp.apply(lambda row: [row['encoded_hero']] + row['encoded_deck'], axis=1)\n",
    "    dfp = dfp[['archetype', 'features']]\n",
    "    dfp.columns = ['label', 'features']\n",
    "    return dfp\n",
    "\n",
    "def prepare_dataset_to_score(dfp, cards_selection, encoder_hero):\n",
    "    dfp['encoded_deck'] = dfp['deck'].apply(lambda deck: encode_deck(deck, cards_selection))\n",
    "    dfp['encoded_hero'] = encoder_hero.transform(dfp['hero'].to_list())\n",
    "    dfp['features'] = dfp.apply(lambda row: [row['encoded_hero']] + row['encoded_deck'], axis=1)\n",
    "    dfp = dfp[['deckid', 'features']]\n",
    "    return dfp\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--test_size', type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print('Received arguments {}'.format(args))\n",
    "    \n",
    "    # Collect the data\n",
    "    input_data_to_train_path = os.path.join('/opt/ml/processing/input/ml', 'dataset_ml.csv')\n",
    "    input_data_to_score_path = os.path.join('/opt/ml/processing/input/toscore', 'dataset_toscore.csv')\n",
    "    print('Reading input data to train from {}'.format(input_data_to_train_path))\n",
    "    dfp_ml = pd.read_csv(input_data_to_train_path)\n",
    "    dfp_ml['deck'] = dfp_ml['cards'].apply(lambda cards:ast.literal_eval(cards))\n",
    "    \n",
    "    # Build the training set and the testing set\n",
    "    dfp_train, dfp_test = train_test_split(dfp_ml, test_size=args.test_size, random_state=0)\n",
    "    dfp_train.reset_index(drop=True,inplace=True)\n",
    "    dfp_test.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # Build some encoder\n",
    "    encoder_hero = preprocessing.LabelEncoder()\n",
    "    heroes = list(dfp_train['hero'].unique())\n",
    "    encoder_hero.fit(heroes)\n",
    "    \n",
    "    # Rank the cards for the deck encoding\n",
    "    dfp_train_agg = dfp_train.groupby(['archetype'])['deck'].apply(list).to_frame()\n",
    "    dfp_train_agg['cards_count'] = dfp_train_agg['deck'].apply(lambda deck: compute_cards_count_v2(deck))\n",
    "    dfp_train_agg['cards_selection'] = dfp_train_agg['cards_count'].apply(lambda cards_count: compute_ranking_cards(cards_count))\n",
    "    cards_selection = list(dict.fromkeys(list(itertools.chain.from_iterable(dfp_train_agg['cards_selection'].tolist()))))\n",
    "    \n",
    "    dfp_dataset_train = prepare_dataset_to_train(dfp_train, cards_selection, encoder_hero)\n",
    "    dfp_dataset_test = prepare_dataset_to_train(dfp_test, cards_selection, encoder_hero)\n",
    "    \n",
    "    print('Reading input data to score from {}'.format(input_data_to_score_path))\n",
    "    dfp_score = pd.read_csv(input_data_to_score_path).sample(frac=0.1).head(1000)\n",
    "    dfp_score['deck'] = dfp_score['cards'].apply(lambda cards:ast.literal_eval(cards))\n",
    "    dfp_dataset_score = prepare_dataset_to_score(dfp_score, cards_selection, encoder_hero)\n",
    "    \n",
    "    # Save the data\n",
    "    print('Saving the data in /opt/ml/processing/train and test and score')\n",
    "    train_output_path = os.path.join('/opt/ml/processing/train', 'dataset_train.csv')\n",
    "    test_output_path = os.path.join('/opt/ml/processing/test', 'dataset_test.csv')\n",
    "    score_output_path = os.path.join('/opt/ml/processing/score', 'dataset_score.csv')\n",
    "    dfp_dataset_train.to_csv(train_output_path, index=None)\n",
    "    dfp_dataset_test.to_csv(test_output_path, index=None)\n",
    "    dfp_dataset_score.to_csv(score_output_path, index=None)\n",
    "    print('DONE')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jobs/1_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jobs/1_train.py\n",
    "\n",
    "\"\"\"\n",
    "Script to process the raw data\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    training_data_directory = '/opt/ml/input/data/train'\n",
    "    print(f'Collect the training set data at {training_data_directory}')\n",
    "    dfp_training = pd.read_csv(training_data_directory + '/dataset_train.csv')\n",
    "    dfp_training['features'] = dfp_training['features'].apply(lambda features: ast.literal_eval(features))\n",
    "    \n",
    "    print('Set the random forest model')\n",
    "    model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    \n",
    "    print('Train the model')\n",
    "    model.fit(dfp_training['features'].tolist(), dfp_training['label'].tolist())\n",
    "    model_output_directory = os.path.join('/opt/ml/model', \"model.joblib\")\n",
    "    \n",
    "    print('Saving model to {}'.format(model_output_directory))\n",
    "    joblib.dump(model, model_output_directory)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jobs/2_evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jobs/2_evaluate.py\n",
    "\n",
    "\"\"\"\n",
    "Script to process the raw data\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import math as mth\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def get_precision_at_k(recommendations, item, k=5):\n",
    "    if item in recommendations[:k]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def get_ndcg_at_k(recommendations, item, k=5):\n",
    "    for idx, elt in enumerate(recommendations[:k]):\n",
    "        if item == elt:\n",
    "            return mth.log(2) / mth.log(idx+2)\n",
    "    return 0\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model_path = os.path.join('/opt/ml/processing/model', 'model.tar.gz')\n",
    "    print('Extracting model from path: {}'.format(model_path))\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path='.')\n",
    "    print('Loading model')\n",
    "    model = joblib.load('model.joblib')\n",
    "    \n",
    "    print('Loading test input data')\n",
    "    dfp_test = pd.read_csv(os.path.join('/opt/ml/processing/test', 'dataset_test.csv'))\n",
    "    dfp_test['features'] = dfp_test['features'].apply(lambda features: ast.literal_eval(features))\n",
    "    \n",
    "    dfp_test['prediction'] = list(model.predict_proba(dfp_test['features'].tolist()))\n",
    "    dfp_test['prediction'] = dfp_test['prediction'].apply(lambda prediction: np.argsort(prediction)[::-1])\n",
    "    \n",
    "    labels = list(model.classes_)\n",
    "    dfp_test['prediction'] = dfp_test['prediction'].apply(lambda prediction: [labels[idx] for idx in prediction])\n",
    "\n",
    "    metrics = []\n",
    "    for k in [1,3,5]:\n",
    "        dfp_test[f'precision_at_{k}'] = dfp_test.apply(lambda row: get_precision_at_k(row['prediction'], row['label'], k), axis=1)\n",
    "        metrics.append(f'precision_at_{k}')\n",
    "        if k > 1:\n",
    "            dfp_test[f'ndcg_at_{k}'] = dfp_test.apply(lambda row: get_ndcg_at_k(row['prediction'], row['label'], k), axis=1)\n",
    "            metrics.append(f'ndcg_at_{k}')\n",
    "    dfp_metrics = dfp_test[metrics].mean().to_frame().reset_index()\n",
    "    dfp_metrics.columns = ['metric', 'value']\n",
    "    evaluation_output_path = os.path.join('/opt/ml/processing/evaluation', 'metrics.csv')\n",
    "    dfp_metrics.to_csv(evaluation_output_path, index=None)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jobs/3_score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jobs/3_score.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model_path = os.path.join('/opt/ml/processing/model', 'model.tar.gz')\n",
    "    print('Extracting model from path: {}'.format(model_path))\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path='.')\n",
    "    print('Loading model')\n",
    "    model = joblib.load('model.joblib')\n",
    "    \n",
    "    print('Loading test input data')\n",
    "    dfp_toscore = pd.read_csv(os.path.join('/opt/ml/processing/score', 'dataset_score.csv'))\n",
    "    dfp_toscore['features'] = dfp_toscore['features'].apply(lambda features: ast.literal_eval(features))\n",
    "    \n",
    "    dfp_toscore['probabilities'] = list(model.predict_proba(dfp_toscore['features'].tolist()))\n",
    "    dfp_toscore['predictions'] = dfp_toscore['probabilities'].apply(lambda prediction_prob: np.argsort(prediction_prob)[::-1])\n",
    "    \n",
    "    labels = list(model.classes_)\n",
    "    dfp_toscore['predictions'] = dfp_toscore['predictions'].apply(lambda predictions: [labels[idx] for idx in predictions])\n",
    "    \n",
    "    evaluation_output_path = os.path.join('/opt/ml/processing/predictions', 'predictions.csv')\n",
    "    dfp_toscore.to_csv(evaluation_output_path, index=None)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
